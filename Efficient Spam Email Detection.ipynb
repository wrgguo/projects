{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Spam Email Detection\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Spam emails can sometimes be annoying and hard to detect when they are nested in mailbox. We have to read them one by one to decide whether we want to remove it or not. Generally it takes a great amount of time to sanitize before the new ones arrive. It would be helpful if we can construct a classifier to detect the spam emails based on our previous effort on removing the spams. In this project, we are going to build and efficiently train a linear SVM spam email classifier from scratch with gradient descent algorithm. The final goal of this project is to make sure that our classifer can acheive at least 80% accuracy on detecting the new spam emails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Before getting started, you'll need to install various packages that will be used in this project. It is strongly recommanded to use Anaconda since all the open source packages can be individually installed from the Anaconda repository. By calling\n",
    "\n",
    "$ pip install [packages you want to install]\n",
    "\n",
    "Or\n",
    "\n",
    "$ conda install [packages you want to install]\n",
    "\n",
    "Anaconda compiles and builds all the packages in the Anaconda repository itself. The packages required are listed as follow.\n",
    "\n",
    "You can also refer to requirements.txt to figure out the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "The emails have already been labeled as 1 if they are ham (not spam), and -1 if they are spam. The lines of the emails have already been slightly processed, such that different words are space delimited, however little other processing has occurred. The data is distributed by course instructor for educational purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.optimize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load the data\n",
    "def load_data(emails_filepath, labels_filepath):\n",
    "    with gzip.open(emails_filepath, 'rt') as f:\n",
    "        emails = f.readlines()\n",
    "    with gzip.open(labels_filepath, 'rt') as f:\n",
    "        labels = np.loadtxt(f)\n",
    "    return emails, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper function to cache the loaded dataset. This helps it not take quite as long to re-run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATA_CACHE\n",
    "DATA_CACHE = None\n",
    "\n",
    "# Load the data\n",
    "def get_data():\n",
    "    global DATA_CACHE\n",
    "    if DATA_CACHE is None:\n",
    "        DATA_CACHE = load_data('X1.txt.gz', 'y1.txt.gz')\n",
    "    return DATA_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a sparse tf-idf matrix where the length of each row represents the vocabulary of the dataset and the number of rows is equal to the number of emails in the dataset. We have 10,000 datapoints but more than 200,000 features for each email, however, most of the emails don't have a specific word. It would be efficient to convert it into sparse format where 0 indicates the word is not contained in this email and 1 vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a sparse tf-idf feature matrix\n",
    "def tfidf(docs):\n",
    "    all_words = set([a for a in \" \".join(docs).split(\" \") if a != \"\"])\n",
    "    all_words_dict = {k:i for i,k in enumerate(all_words)}\n",
    "    word_counts = [Counter([a for a in d.split(\" \") if a != \"\"]) for d in docs]\n",
    "\n",
    "    data = [a for wc in word_counts for a in wc.values()]\n",
    "    rows = [i for i,wc in enumerate(word_counts) for a in wc.values()]\n",
    "    cols = [all_words_dict[k] for wc in word_counts for k in wc.keys()]\n",
    "    # Convert it into sparse\n",
    "    X = sp.coo_matrix((data, (rows,cols)), (len(docs), len(all_words)))\n",
    "\n",
    "    idf = np.log(float(len(docs))/np.asarray((X > 0).sum(axis=0))[0])\n",
    "    return X * sp.diags(idf), list(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification\n",
    "We are going to build a linear SVM classifier in this section. In general, all the machine learning problem can be roughly divided into the following three sections in terms of mathematic interpretation.\n",
    "\n",
    "1. Hypothesis function: A hypothesis funtion is a mapping from the input space to the prediction space.\n",
    "2. Loss function: A mapping from predictions and true outputs to real numbers, is a measure of how good a prediction is. In our case, SVM is using hinge loss.\n",
    "3. Optimization: Minimize the loss function using different algorithms (in our case we use gradient descent)\n",
    "\n",
    "The format of objective function (also known as loss function) to implement is:$$\n",
    "\\mathcal L(y, X, \\theta, \\lambda) = \\text{sum}\\left\\{\\text{max}\\{1 - y\\cdot \\theta^T X, 0 \\}\\right\\} + \\frac\\lambda2(\\theta\\cdot\\theta)\n",
    "$$ where $X$ is the input, $y$ is the true label, $\\theta$ is the model weight, and $\\lambda$ is the regularization weight. The inner `max` operation is performed over each element, and the outer `sum` operation returns the sum of the elements of the vector.\n",
    "\n",
    "The gradient is:$$\n",
    "    v = \\mathbb1\\left\\{y* (X \\theta)) <= 1\\right\\}\n",
    "\\\\    \n",
    "\\\\    \\frac{d\\theta}{d\\mathcal L} = \\text{sum}_\\text{col}\\{-y* v*X\\} + \\lambda\\theta\n",
    "$$\n",
    "where the $\\text{sum}_\\text{col}$ operation calculates the sum over each column, producing a vector the same shape as $\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    # Initialize the SVM attributes and initialize the weights vector to the zero vector. \n",
    "    def __init__(self, X, y, reg):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.reg = reg\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    # Calculate the objective value of the SVM。\n",
    "    def objective(self, X, y):\n",
    "        return np.maximum(np.ones(self.X.shape[0]) - self.theta @ self.X.T * self.y, 0).sum() + (self.reg / 2) * self.theta @ self.theta.T\n",
    "    \n",
    "    \n",
    "    # Calculate the gradient of the objective value on the training examples. \n",
    "    def gradient(self):\n",
    "        v = np.where((self.y * (self.X @ self.theta)) <= 1, 1, 0)\n",
    "        tmp = (self.X.T * sp.diags((-self.y * v),0)).T.sum(axis=0) + self.reg * self.theta\n",
    "        try:\n",
    "            return tmp.A1\n",
    "        except:\n",
    "            return tmp\n",
    "    \n",
    "    # Train the support vector machine with the given parameters. \n",
    "    def train(self, niters=100, learning_rate=1, verbose=False):\n",
    "        for t in range(niters):\n",
    "            self.theta -= learning_rate * self.gradient()\n",
    "\n",
    "    # Predict the class of each label in X. \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we want to use matrix operation (especially sparse matrix) here is that it is way much faster than mathematical calculation using for loops. The performance of our SVM classifier determines whether we can efficiently detect the spam emails. Let's try to time our training function to see how long does it takes train our email dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78 s ± 40 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Train SVM on tfidf features using the labeled email dataset\n",
    "emails, labels = get_data()\n",
    "features, all_words = tfidf(emails)\n",
    "svm = SVM(features, labels, 1e-4)\n",
    "svm.train(verbose=True)\n",
    "\n",
    "# Uncomment the line below to see how long training takes to run. \n",
    "# %timeit svm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that fast enough to handle such a big dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Cross validation and Parameter Grid Search \n",
    "We might have noticed that there are parameters in the SVM learning algorithm that we chose somewhat arbitrarily: the regularization parameter and the learning rate\n",
    "\n",
    "We can achieve perfect training accuracy with these random parameters. But since we have an enormous amount of features so it would be extremely easy to overfit to the data, so our model may not generalize well. \n",
    "\n",
    "We are now going to evaluate and select these parameters using cross validation and grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation\n",
    "There are many ways to evaluate the choice of parameters. One way is to perform k-fold cross validation, which operates as follows \n",
    "\n",
    "1. Split the data into k+1 randomly selected but uniformly sized pieces, and set aside one block for testing.\n",
    "2. For each of the remaining k parts, train the model on k-1 parts and validate the model on the heldout part. \n",
    "3. This gives k results, and the average of these runs gives the final result.\n",
    "\n",
    "The idea is that by holding out part of the dataset as validation data, we can train and measure our generalization ability. The training does not see the validation data at all, which is why it measures generalization. Randomizing the groups removes bias from ordering, and averaging over the groups reduces the variance. \n",
    "\n",
    "In this problem, we will use classification error rate as our result metric (so the fraction of times in which our model returns the wrong answer). Calculating this value via k-fold cross validation gives us a measure of how well our model generalizes to new data (lower error rate is better). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "Now, we have a means of evaluating our choice of parameters. We can now combine this with a grid search over parameters to determine the best combination. Given two lists of parameters, we compute the classification error using k-fold cross validation for each pair of parameters, and output the parameters that produces the best validation result. We are testing on learning rates [0.1, 1, 10] and regularization [0.01, 0.1, 1, 10] with 100 iterations and k=5. However, you can adjust the all these parameters based on practical need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelSelector:\n",
    "    \n",
    "    # Initialize the model selection with data and split into train/valid/test sets. Split the permutation into blocks \n",
    "    # and save the block indices as an attribute to the model. \n",
    "    def __init__(self, X, y, P, k, niters, SVM_impl):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.P = P\n",
    "        self.k = k\n",
    "        self.niters = niters\n",
    "        self.svm = SVM_impl\n",
    "        n = int(np.ceil(len(P)/(k+1)))\n",
    "        blocks = [P[i:i+n] if i+n < len(P) else P[i:] for i in range(0, len(P), n)]\n",
    "        self.blocks = blocks[:k]\n",
    "        self.test_block = blocks[-1]\n",
    "\n",
    "        \n",
    "    # Evaluate the SVM using k-fold cross validation for the given parameters \n",
    "    def cross_validation(self, lr, reg):\n",
    "        avg_err = []\n",
    "        num = len(self.blocks)\n",
    "        for i in range(num):\n",
    "            train_rows = np.asarray([v for sub in range(num) if sub != i for v in self.blocks[sub]])\n",
    "            valid_X = self.X[self.blocks[i],:]\n",
    "            train_X = self.X[train_rows,:]\n",
    "            valid_y = self.y[self.blocks[i]]\n",
    "            train_y = self.y[train_rows]\n",
    "            \n",
    "            model = self.svm(train_X, train_y, reg)\n",
    "            model.train(learning_rate=lr)\n",
    "            pred_y = model.predict(valid_X)\n",
    "            err = np.mean(pred_y*valid_y < 0)\n",
    "            avg_err.append(err)\n",
    "        \n",
    "        return np.mean(avg_err)\n",
    "\n",
    "    # Given two lists of parameters for learning rate and regularization parameter, perform a grid search using\n",
    "    # k-wise cross validation to select the best parameters. \n",
    "    def grid_search(self, lrs, regs):\n",
    "        import itertools\n",
    "        best_lr = None\n",
    "        best_reg = None\n",
    "        best_err = None\n",
    "        combs = list(itertools.product(lrs, regs))\n",
    "        for comb in combs:\n",
    "            lr, reg = comb[0], comb[1]\n",
    "            curr_err = self.cross_validation(lr, reg)\n",
    "            if best_err == None or curr_err < best_err:\n",
    "                best_err = curr_err\n",
    "                best_lr = lr\n",
    "                best_reg = reg\n",
    "        return (best_lr, best_reg)\n",
    "    \n",
    "\n",
    "    # Calculate the error rate of the test data given the rest of the data.      \n",
    "    def test(self, lr, reg):\n",
    "        train_rows = np.asarray([v for sub in range(len(self.blocks)) for v in self.blocks[sub]])\n",
    "        train_X = self.X[train_rows,:]\n",
    "        train_y = self.y[train_rows]\n",
    "        test_X = self.X[self.test_block,:]\n",
    "        test_y = self.y[self.test_block]\n",
    "        model = self.svm(train_X, train_y, reg)\n",
    "        model.train(learning_rate=lr)\n",
    "        pred_y = model.predict(test_X)\n",
    "        err = np.mean(pred_y*test_y < 0)\n",
    "        return (err, model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement these functions on our email dataset to interpret how each method works. First let's import our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x248087 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2364991 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and construct tfidf matrix\n",
    "emails, labels = get_data()\n",
    "features, all_words = tfidf(emails)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the matrix is very sparse, as mentioned at the beginning of the note. Then we permutate the order of datapoints to ensure the randomness in cross validation. We use k=5, iterations =100, learning rate = 0.5 and regularization = 0.1 to test the functionality of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010197960407918417"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign random perturbation\n",
    "P = np.random.permutation(features.shape[0])\n",
    "\n",
    "# Initialize the model selector\n",
    "MS = ModelSelector(features, labels, P, 5, 100, SVM)\n",
    "\n",
    "# Caculate the classification error rate\n",
    "cv_err = MS.cross_validation(0.5, 0.1)\n",
    "assert(cv_err < 0.05)\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to perform grid search on learning rates and regularization. The grid search function will return the selected combination from learning rates [0.1, 1, 10] and regularization [0.01, 0.1, 1, 10] which gives us the lowest classification error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr, reg = MS.grid_search(np.array([0.001, 1., 10.]), np.array([0.0001, 0.1, 1.]))\n",
    "lr,reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Compression \n",
    "While we can get decent results using an SVM and basic tf-idf features, there is one problem that we have to notice:\n",
    "1. The number of features is extremely bloated and consumes a lot of space and computing power for a binary classification problem\n",
    "\n",
    "So the above methodology would actually take a lot of time and memory to run on the full dataset. We save the tf-idf matrix for the entire training dataset (which is enormous), and then use that to generate features on new examples. \n",
    "\n",
    "One way to tackle this is to generate fewer, but effective, features. For example, instead of generating full tf-idf features for every single word in every email, we can instead try to focus on keywords that frequently only occur in spam email.\n",
    "\n",
    "The implementation process is indicated as follow:\n",
    "\n",
    "1. Split each document `d` into words using `d.split()`.\n",
    "2. Determine the frequency with which words occur in spam and ham (not spam) emails. Count each appearance of each word.\n",
    "3. Select words that only ever appear in spam and words that only ever appear in ham\n",
    "4. From these words, return sets of those that appear at least `threshold` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of emails and corresponding labels, return a list of frequent indicator words \n",
    "# for spam emails and for ham emails. \n",
    "def find_frequent_indicator_words(docs, y, threshold):\n",
    "    words = [tuple(email.split()) for email in docs]\n",
    "    labels = list(y)\n",
    "    corr_list = list(zip(words, labels))\n",
    "    spams = [w for i in corr_list if i[1] == -1.0 for w in i[0]]\n",
    "    hams = [w for i in corr_list if i[1] == 1.0 for w in i[0]]\n",
    "    spams_count = dict(Counter(spams))\n",
    "    hams_count = dict(Counter(hams))\n",
    "    spams_words, hams_words = set(spams_count.keys()), set(hams_count.keys())\n",
    "    spams_only = [(w, spams_count[w]) for w in spams_words if w not in hams_words]\n",
    "    hams_only = [(w, hams_count[w]) for w in hams_words if w not in spams_words]\n",
    "    spams_freq = [item[0] for item in spams_only if item[1] >= threshold]\n",
    "    hams_freq = [item[0] for item in hams_only if item[1] >= threshold]\n",
    "    return (spams_freq, hams_freq)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass threshold as 50 to filter out those words that don't occur frequently. Let's see how great it can reduce the dimension of our feature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2422, 290)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails, labels = get_data()\n",
    "spam,ham = find_frequent_indicator_words(emails, labels, 50)\n",
    "len(spam), len(ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our original feature matrix has more than 200,000 columns. This is a great reduction and save a lot of space and computation power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Spam Detection\n",
    "\n",
    "Our final goal is to get at least 80% accuracy on spam email detection in an efficient manner. We will use the frequent indicator words implemented above and generate 2 features per email: the number of spam indicator words and the number of ham indicator words for a total of two features. As we metioned above, this is a huge dimensionality reduction.\n",
    "\n",
    "Given the frequent indicator words, we will first generate 2 features per email for a given training dataset. Then, we will use ModelSelector to perform a grid search and train SVM classifier on the training dataset. Finally, we will evaluate the trained SVM on a separate test set, where we aim to achieve at least 80% accuracy.  \n",
    "\n",
    "The function below generates the new email features based on the result of frequent indicator words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of emails, create a matrix containing features for each email.\n",
    "def email2features(emails, frequent_indicator_words):\n",
    "    spams_freq, hams_freq = set(frequent_indicator_words[0]), set(frequent_indicator_words[1])\n",
    "    spams_list, hams_list = [], []\n",
    "    counter_list = [dict(Counter(email.split())) for email in emails]\n",
    "    for item in counter_list:\n",
    "        spams_dict, hams_dict = {}, {}\n",
    "        for k in item:\n",
    "            if k in spams_freq:\n",
    "                spams_dict[k] = item[k]\n",
    "            if k in hams_freq:\n",
    "                hams_dict[k] = item[k]\n",
    "        spams_list.append(spams_dict)\n",
    "        hams_list.append(hams_dict)\n",
    "    spams_list = [sum(item.values()) for item in spams_list]\n",
    "    hams_list = [sum(item.values()) for item in hams_list]\n",
    "    output = np.array(list(zip(spams_list, hams_list)))\n",
    "    \n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final detection function is implemented as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.131"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def efficient_spam_detection_test(efficient_spam_detection_impl):\n",
    "    emails, labels = get_data()\n",
    "    emails_tr, labels_tr = load_data('Xtr_spam.txt.gz', 'ytr_spam.txt.gz')\n",
    "\n",
    "    frequent_indicator_words = find_frequent_indicator_words(emails, labels, 50)    \n",
    "    svm = efficient_spam_detection_impl(emails_tr, labels_tr, frequent_indicator_words)\n",
    "    \n",
    "    # Training error\n",
    "    features = email2features(emails_tr, frequent_indicator_words)\n",
    "    yp = svm.predict(features)\n",
    "    print(np.mean(yp * labels_tr < 0))\n",
    "    test.true(np.mean(yp * labels_tr < 0) <= 0.2)\n",
    "\n",
    "# Given a training dataset of emails and labels, use the given frequent indicator words to\n",
    "# generate features per email. Use ModelSelector to perform a grid search and train SVM\n",
    "# on the training dataset\n",
    "def efficient_spam_detection(emails, labels, frequent_indicator_words):\n",
    "    \n",
    "    features = email2features(emails, frequent_indicator_words)\n",
    "    P = np.random.permutation(features.shape[0])\n",
    "    MS = ModelSelector(features, labels, P, 5, 100, SVM)\n",
    "    \n",
    "    lr, reg = MS.grid_search(np.array([0.001, 1., 10.]), np.array([0.0001, 0.1, 1.]))\n",
    "    svm = SVM(features, labels, reg)\n",
    "    svm.train(niters=100, learning_rate=lr, verbose=False)\n",
    "    return svm\n",
    "\n",
    "emails, labels = get_data()\n",
    "emails_tr, labels_tr = load_data('Xtr_spam.txt.gz', 'ytr_spam.txt.gz')\n",
    "\n",
    "frequent_indicator_words = find_frequent_indicator_words(emails, labels, 50)    \n",
    "svm = efficient_spam_detection(emails_tr, labels_tr, frequent_indicator_words)\n",
    "\n",
    "# Training error\n",
    "features = email2features(emails_tr, frequent_indicator_words)\n",
    "yp = svm.predict(features)\n",
    "\n",
    "\n",
    "np.mean(yp * labels_tr < 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Finally we achieve our goal to reach accuracy of at least 80% in detecting spam emails. Notice that we are not predicting on our original dataset, instead we compress our features only selecting those words occurring frequently. It might seem to sacrifice the integrity of information in the dataset at first glance, but the actual functionality of those infrequent words can be limited. We can adjust the threshold to determine how we want to define \"frequent\". Also, there are lots of optimization and trade off options going on in this project, we have full flexibility over the learning rate, regularization and iterations to decide how we want our SVM classifier to work. One of the things that we can optimize is to do more pre-processing on our raw email dataset. What we have done is just separating each word with white space. More techniques (like regular expression) can be done to filtering the raw data based on practical need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scipy: https://www.scipy.org/\n",
    "2. Numpy: https://numpy.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
