{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Prediction By Tweets Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Twitter is regarded as one of the major resources of free text. In addition to the more structured relational data, free text makes up one of the most common types of widely available data: web pages, unstructured “comment” fields in many relational databases, and many other easily-obtained large sources of data naturally come in free text form. However, free text, unlike numeric data, by its very definition usually needs to have meaning to the people who are reading that data. It might be good to think of a way of analyzing free text in a more objective way as we usually deal with numeric data. Natural Language Processing (NLP) provides us a good way to extract and generalize information from free text. Methods like bag-of-word models, TFIDF vectors, and (simple n-gram) language models are widely used in processing raw text information from social media like Twitter and many other fields.\n",
    "\n",
    "This project intends to analyze the Twitter data extracted using [this](https://dev.twitter.com/overview/api) api. Some data preprocessing has already been done. Only tweets posted by the following six Twitter accounts are kept: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`.\n",
    "\n",
    "For every tweet, only two pieces of information are kept:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "The training set contains both the `screen_name` and `text` of each tweet; the test set only contains the `text`.\n",
    "\n",
    "The overarching goal of the project is to infer the political inclination (whether **R**epublican or **D**emocratic) of the author from the tweet text. The ground truth (i.e., true class labels) are determined from the `screen_name` of the tweet as follows:\n",
    "- **R**: `realDonaldTrump, mike_pence, GOP`\n",
    "- **D**: `HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "Therefore, the problem can be treated as a binary classification. The analysis structure is indicated as follows:\n",
    "\n",
    "1. **preprocessing**: clean up the raw tweet text using the various functions offered by [the Natural Language Toolkit (`nltk`)](http://www.nltk.org/genindex.html).\n",
    "2. **features**: construct bag-of-words feature vectors.\n",
    "3. **classification**: learn a binary classification model using [`scikit-learn`](http://scikit-learn.org/stable/modules/classes.html). \n",
    "\n",
    "Note that `nltk` supports optional corpora, toy grammars, trained models, etc. we also have to manually install the stopwords list (high frequency words in English) and `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, you'll need to install various packages that will be used in this project. It is strongly recommanded to use Anaconda since all the open source packages can be individually installed from the Anaconda repository. By calling\n",
    "\n",
    "   $ pip install [packages you want to install]\n",
    "   \n",
    "Or\n",
    "\n",
    "   $ conda install [packages you want to install]\n",
    "   \n",
    "Anaconda compiles and builds all the packages in the Anaconda repository itself. The packages required are listed as follow.\n",
    "\n",
    "You can also refer to requirements.txt to figure out the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DavidGuo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DavidGuo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DavidGuo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gzip\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "\n",
    "# Install the required pacakges\n",
    "try:\n",
    "    lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    stopwords=nltk.corpus.stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Installation Failed\")\n",
    "        \n",
    "\n",
    "def nltk_download():\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "nltk_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the raw tweets, we notice that the tweets are not only formatted with alphanumeric characters (also including special characters). Some of them may not have semantic meaning. Therefore we need to lemmatize and tokenize the raw tweets into separate word string for further analysis. Here we predefine a set of rule of thumb on what a \"meaningful\" word actually means. The basic principles of tokenizing tweets are indicated as follows: \n",
    "\n",
    "The tokens must:\n",
    "\n",
    "1. be in lower case.\n",
    "2. appear in the same order as in the raw text.\n",
    "3. be in their lemmatized form, if one exists. If a word cannot be lemmatized, do not include it in the output.\n",
    "4. **not** contain any characters other than numbers and digits.\n",
    "   1. remove trailing `'s`: `Children's` becomes `children`\n",
    "   2. omit other apostrophes: `don't` becomes `dont`\n",
    "   3. break tokens at other punctuation and/or unicode characters: `word-of-mouth` becomes `word`, `of`, `mouth` \n",
    "5. if the lemmatized form is a stopword, it should not appear in the output\n",
    "6. not include the parts of any t.co urls. Many tweets contain URLs from the domain `t.co`; All URLs with `t.co` domain should be striped.\n",
    "\n",
    "**Stopwords** are words that appear very often in text, usually playing a grammatical role (\"and\", \"a\", etc.). When comparing text similarity, these are not very useful; so we eliminate them at this stage. (NLTK provides us with a list of stopwords for English.)\n",
    "\n",
    "We also don't want to include stopwords in our tokens, we make it as an optional parameter in our function so that we can expand its meaning in addition to the stopwords for English provided NLTK package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to tokenize word string by row readed from csv\n",
    "def preprocess(text, stopwords={}, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    # handle URL\n",
    "    text = re.sub(r'(http|https)://t.co/[a-zA-Z0-9]{10}', ' ', text)\n",
    "    \n",
    "    # remove trailing \"'s\"\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    \n",
    "    # remove trailing \"'S\"\n",
    "    text = text.replace(\"'S\", \"\")\n",
    "    \n",
    "    # omit other apostrophes\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    \n",
    "    # replace with whitespace\n",
    "    for c in text:\n",
    "        if c in string.ascii_letters or c in string.digits:\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(c, \" \")\n",
    "    \n",
    "    # to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenize with whitespace\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # lemmatize each word in the text list\n",
    "    text = [lemmatizer.lemmatize(w) for w in text]\n",
    "    \n",
    "    # remove the word in the stoplist\n",
    "    # stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    text = [w for w in text if w not in stopwords]\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `preprocess` function is used to tokenize the word string by row readed from csv file. The following functions implemented the `preprocess` function defined above to tokenize the tweets row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the csv file and map the preprocess function to each csv row\n",
    "def read_csv(stem, process=lambda x: x):\n",
    "    with gzip.open(f\"{stem}.csv.gz\", \"rt\", newline='', encoding=\"UTF-8\") as file:\n",
    "        csvr = csv.reader(file)\n",
    "        next(csvr)\n",
    "        return list(map(process, csvr))\n",
    "\n",
    "def is_republican(r):\n",
    "    return r in [\"realDonaldTrump\", \"mike_pence\", \"GOP\"]\n",
    "\n",
    "# This function processes the whole dataset and allows customized stopwords \n",
    "def read_data(extra_stopwords=set()):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"]) | extra_stopwords\n",
    "    data_train = read_csv(\"tweets_train\", process=lambda r: (is_republican(r[0]), preprocess(r[1], stopwords)))\n",
    "    data_test = read_csv(\"tweets_test\", process=lambda r: preprocess(r[0], stopwords))\n",
    "    \n",
    "    \"\"\"Reads the dataset from the csv.gz files\n",
    "    \n",
    "    return : Tuple[data_train, data_test]\n",
    "        data_train : List[Tuple[is_republican, tokenized_tweet]]\n",
    "            is_republican : bool -- True if tweet is from a republican\n",
    "            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n",
    "    \"\"\"\n",
    "    \n",
    "    return (data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our processed training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " ['gopconvention',\n",
       "  'oregon',\n",
       "  'vote',\n",
       "  'today',\n",
       "  'mean',\n",
       "  '62',\n",
       "  'day',\n",
       "  'gopconvention'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data_train, p_data_test = read_data()\n",
    "p_data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"True\" symbol means that the author of the tweet text inclines to Republican, and vice vera. The followed list of words is a combination of \"meaningful\" words defined by our previous principles. We can also inspect our testing dataset. The only difference is that it lacks \"True\" or \"False\" symbols. We need to predict it using the provided tweets text in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comprehensive', 'look', 'many', 'lie', 'offense', 'donald', 'trump']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Construction\n",
    "\n",
    "Before we jump into the feature construction section, a few terminologies should be noted:\n",
    "\n",
    "1. `document`: an individual group of free text data (in our case is each row of tweet)\n",
    "2. `corpus`: a collection of documents\n",
    "3. `vocabulary`: a set of all unique words/tokens in the corpus\n",
    "\n",
    "The next step is to derive feature vectors from the tokenized tweets. In this section, we will construct a bag-of-words [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature vector.\n",
    "\n",
    "The number of possible words is prohibitively large, and not all words are useful for our task. We will begin by filtering the vectors using a common heuristic:\n",
    "\n",
    "We calculate a frequency distribution of words in the corpus, and remove words at the head (most frequent) and tail (least frequent) of the distribution. Most frequently used words (often called stopwords) provide very little information about the similarity of two pieces of text; we have already removed these. Words with extremely low frequency tend to be typos.\n",
    "\n",
    "We will now implement a function which counts the number of times that each token is used in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A helper function to plot the distribution\n",
    "def get_distribution_helper(get_distribution):\n",
    "    data_train, data_test = read_data()\n",
    "    dist = get_distribution(data_train)\n",
    "    if dist is None:\n",
    "        return\n",
    "\n",
    "    plt.hist(dist.values(), bins=100)\n",
    "    plt.yscale('log')\n",
    "\n",
    "# A function to count the frequency of words\n",
    "def get_distribution(data_train):\n",
    "    lst = [ele[1] for ele in data_train]\n",
    "    lst = [i for j in lst for i in j]\n",
    "    counter = collections.Counter(lst)\n",
    "    return counter\n",
    "    \"\"\" Calculates the word count distribution, excluding stopwords.\n",
    "\n",
    "    args: data_train -- the training data\n",
    "\n",
    "    return : collections.Counter -- the distribution of word counts\n",
    "    \"\"\"\n",
    "get_distribution_helper(get_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the distribution looks exponential, even with a logarithmic y-axis. There are a lot words that appear only once, so we need to figure out what these words are and eliminate them from the dataset accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rare_words(dist):\n",
    "    new_stopwords = set()\n",
    "    for word in dist:\n",
    "        if dist[word] == 1:\n",
    "            new_stopwords.add(word)\n",
    "            \n",
    "    return new_stopwords\n",
    "    \n",
    "    \"\"\"use the word count information from the training data to find more stopwords\n",
    "\n",
    "    args: dist: collections.Counter -- the output of get_distribution\n",
    "\n",
    "    returns : Set[str] -- a set of all words that appear exactly once in the training data\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might notice that in the previous steps each time we call the preprocess function, it takes a few seconds to run. Here we provide a wrapper function to cache the preprocessed data. This helps it not take quite as long to re-run. If you change anything above this cell, re-run this cell to clear the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global PREPROCESSED_DATA_CACHE\n",
    "PREPROCESSED_DATA_CACHE = None\n",
    "\n",
    "def get_data():\n",
    "    global PREPROCESSED_DATA_CACHE\n",
    "    if PREPROCESSED_DATA_CACHE is None:\n",
    "        data_train, data_test = read_data()\n",
    "        dist = get_distribution(data_train)\n",
    "        new_stopwords = get_rare_words(dist)\n",
    "        PREPROCESSED_DATA_CACHE = read_data(new_stopwords)\n",
    "\n",
    "    return PREPROCESSED_DATA_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "\n",
    "Now we have each tweet as a list of words, excluding words with high- and low-frequencies. We want to convert these into a sparse feature matrix, where each row corresponds to a tweet and each column to a possible word. We can use `scikit-learn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to do this quite easily.\n",
    "\n",
    "Note that though: `TfidfVectorizer` expects the input to be a string, and (by default) it perfoms its own analyzing. We have to override that behavior by passing in `do_nothing` to the constructor as an optional parameter.\n",
    "\n",
    "The vectorizing method uses [scikit-learn vectorizer classes](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes) and refers to a [tech blog](http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, will be passed twice\n",
    "def do_nothing(x):\n",
    "    return x\n",
    "\n",
    "def create_features(train_data, test_data):\n",
    "\n",
    "    train_labels = None\n",
    "    train_features = None\n",
    "    test_features = None\n",
    "    \n",
    "    train_doc = [ele[1] for ele in train_data]\n",
    "    train_labels = np.array([ele[0] for ele in train_data])\n",
    "    \n",
    "    \n",
    "    tfidf = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = do_nothing,\n",
    "    preprocessor = do_nothing,\n",
    "    token_pattern = None)\n",
    "    \n",
    "    tfidf.fit(train_doc)\n",
    "    test_features = tfidf.transform(test_data)\n",
    "    train_features = tfidf.transform(train_doc)\n",
    "    return (train_features, train_labels, test_features)\n",
    "    \n",
    "    \"\"\"creates the feature matrices and label vector for the training and test sets.\n",
    "\n",
    "    args:\n",
    "        train_data : List[Tuple[is_republican, tweet_words]]\n",
    "        is_republican : bool -- True if Republican, False otherwise\n",
    "        tweet_words : List[str] -- the processed tweet tokens\n",
    "        test_data : List[List[str]] -- a list of processed tweets\n",
    "\n",
    "    returns: Tuple[train_features, train_labels, test_features]\n",
    "        train_features : scipy.sparse.csr.csr_matrix -- feature matrix for the training set\n",
    "        train_labels : np.array[num_train] -- a numpy vector, where 1 stands for Republican and 0 stands for Democrat \n",
    "        test_features : scipy.sparse.csr.csr_matrix -- feature matrix for the test set\n",
    "    \"\"\"\n",
    "train_features, train_labels, test_features = create_features(*get_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17298x8714 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 161480 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x8714 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9037 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the created matrices are very sparse, indicating that some words seldom appear in most of the tweets.\n",
    "\n",
    "Now that we have the features, the next step will be the classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "In this section, we are going to put it all together and train the classification model.\n",
    "\n",
    "We use the Support Vector Machine [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) to build the classifier.\n",
    "\n",
    "At the heart of an SVM is the concept of a _kernel function_, which determines the distance between two data points. The hypothesis function of SVM, which maps our inputs to the prediction fields, can refer either to a linear hypothesis function or a particular form of nonlinear hypothesis known as a kernal hypothesis.  `sklearn.svm.SVC` natively supports four kernel functions: `linear`, `poly`, `rbf`, `sigmoid`. For this problem, we will use the `linear` kernel.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. build a classifier using the `linear` kernel,\n",
    "2. train it using the training set,\n",
    "3. evaluate the trained model on the training set (might be better to use a validation set), and then\n",
    "4. use it to predict classification on our test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to build a linear classifier\n",
    "def learn_classifier(train_features, train_labels, kernel=\"linear\"):\n",
    "\n",
    "\n",
    "    model = SVC(kernel='linear', gamma = 'auto')\n",
    "    model.fit(train_features, train_labels)\n",
    "        \n",
    "    return model\n",
    "\n",
    "    \"\"\"learns a classifier from the input features and labels using a specified kernel function\n",
    "\n",
    "    args:\n",
    "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        train_labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n",
    "\n",
    "    return : sklearn.svm.classes.SVC -- classifier\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained a classifier, the next step is to measure its performance. This step is necessary to select the best model among a given set of models, or even tune hyperparameters for a given model.\n",
    "\n",
    "F1 score is one of the statistical ways to measure the accuracy of our classifications. F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. Here is a link to wiki of the description of [$F 1$ score](https://en.wikipedia.org/wiki/F1_score).\n",
    "\n",
    "The following functions implement the calculation of F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(pred, ground):\n",
    "\n",
    "    pred = np.array(pred, dtype=bool)\n",
    "    ground = np.array(ground, dtype=bool)\n",
    "\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    \n",
    "    for i, v in enumerate(ground):\n",
    "        if ground[i]:\n",
    "            if ground[i] == pred[i]:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if ground[i] == pred[i]:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    \n",
    "\n",
    "    precision = TP / (TP+FP)\n",
    "    recall = TP / (TP+FN)\n",
    "    F1 = 2 * (precision*recall) / (precision+recall)\n",
    "    return F1\n",
    "\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "\n",
    "    args:\n",
    "        pred: numpy.ndarray(bool) -- predictions\n",
    "        ground: numpy.ndarray(bool) -- known ground-truth values\n",
    "    \n",
    "    return : double -- the F1 score of the predictions\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the F1 score on our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(evaluate):\n",
    "    train_features, train_labels, _ = create_features(*get_data())\n",
    "    test.true(np.abs(evaluate(train_features, train_labels, 'linear') - 0.9538984242282234) < 1e-5)\n",
    "\n",
    "\n",
    "def evaluate(train_features, train_labels, kernel=\"linear\"):\n",
    "    model = learn_classifier(train_features, train_labels, kernel=\"linear\")\n",
    "    pred_labels = model.predict(train_features)\n",
    "    \n",
    "    F1 = f1(pred_labels, train_labels)\n",
    "    return F1\n",
    "    \"\"\"train the classifier and report the F1 score on the training set\n",
    "    \n",
    "    args:\n",
    "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        train_labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n",
    "\n",
    "    return : double -- the F1 score of the predictions on the training labels\n",
    "    \"\"\"\n",
    "train_features, train_labels, _ = create_features(*get_data())\n",
    "f1score = evaluate(train_features, train_labels, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538984242282234"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score of our trained classifier approaches 1, indicating that we have a relatively good classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Test Tweets\n",
    "\n",
    "The final step would be using our trained classifier to classify the test tweets. Use `learn_classifier` to make a trained classifier and predict the labels given the `test_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td><b>D</b></td><td>A comprehensive look at the many lies and offenses of Donald Trump: https://t.co/HKY6HxxFUX https://t.co/cF5GsywU3f                            </td></tr>\n",
       "<tr><td><b>D</b></td><td>\"I’m here as a proud American, a proud Democrat, a proud mother, and tonight, in particular, a very, very proud daughter.” —@ChelseaClinton    </td></tr>\n",
       "<tr><td><b>R</b></td><td>Oops! Clinton confuses the Constitution with the Declaration of Independence &amp; backs a constitutional right to life.\n",
       "https://t.co/gG6xbptUyo                                                                                                                                                </td></tr>\n",
       "<tr><td><b>R</b></td><td>Secret Server you need to wipe clean? http://t.co/oHlxKqImWB Get Hillary's Secret Server Wiper today. http://t.co/ANbo9R6Qwt                   </td></tr>\n",
       "<tr><td><b>D</b></td><td>\"My dad ran a union ironworking shop...my mom was his best salesman. My brothers &amp; I pitched in...that's how small family businesses do it\"</td></tr>\n",
       "<tr><td><b>D</b></td><td>Thomas Jefferson loved vanilla ice cream. He brought home a recipe from France, which is now in the @librarycongress #VAisForPresidents        </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the result in tabular format\n",
    "def pp(entries):\n",
    "    display(HTML(tabulate.tabulate([(f'<b>{\"R\" if isr else \"D\"}</b>', txt[0]) for isr, txt in entries], tablefmt='html')))\n",
    "\n",
    "\n",
    "def classify_tweets(train_features, train_labels, test_features):\n",
    "    model = learn_classifier(train_features, train_labels, kernel=\"linear\")\n",
    "    pred_labels = model.predict(test_features)\n",
    "                                     \n",
    "    return pred_labels\n",
    "    \"\"\"Train a model and predict class labels for the test set.\n",
    "\n",
    "    args:\n",
    "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        train_labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        test_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features, test set\n",
    "\n",
    "    return : numpy.ndarray[bool] -- True if the corresponding tweet is predicted to be Republican, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "test_original = read_csv(\"tweets_test\")\n",
    "train_features, train_labels, test_features = create_features(*get_data())\n",
    "test_classes = classify_tweets(train_features, train_labels, test_features)\n",
    "                                     \n",
    "# Pick up several results from the prediction set\n",
    "pp([e for i, e in enumerate(zip(test_classes, test_original)) if i in [0, 2, 9, 70, 654, 723]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pick up several tweets result from our 1,000 testing dataset. The left column shows our prediction results (\"D\" means Democratic, \"R\" means Republican). The right column is the original raw testing tweets in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabular format results above clearly indicate whether a tweet shows inclination to Republican or Democratic. And we can also identify how messy the orginal raw text can be without text preprocessing. Based on our original purpose of this project, we predefine the ground truth of inclination on the user name of Twitter account. It would be more objective if we can conduct survey on those users to determine their real inclinations, but not just deducting from the user names. In such way we can to some extent ensure the quality of our training dataset. Also notice that the project can be extended to a multiclass classification problem if we have a third or more choices of inclinations in addition to Republican and Democratic. \"True\" or \"False\" may not be enough to identify the inclinations instead we may want some identifiers like \"A\", \"B\" and \"C\" etc when we develop our text preprocessing procedure. The future effort of this project will include a n-gram language model to better predict the inclination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Nltk: http://www.nltk.org/genindex.html\n",
    "2. Numpy: https://numpy.org/\n",
    "3. Scikit-learn: https://scikit-learn.org/stable/\n",
    "4. Matplotlib: https://matplotlib.org\n",
    "5. Tabulate: https://pypi.org/project/tabulate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
