{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income Estimate By Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "If we are given some collection data, how can we find a distribution (or more specifically, find the parameters of a particular class of distribution), that fit this data \"well\". One of the common solutions is to use maximum likelihood estimation. The basic idea of maximum likelihood estimation, is that we want to pick parameters θ that maximize the probaiblity of the observed data; in other words, we want to choose θ to solve the optimization problem. It can serve as a nice principle for how we fit parameters of distributions to data.\n",
    "\n",
    "One simple algorithm that results from the integration of maximum likelihood estimation techniques is the Naive Bayes algorithm for classification. Naive Bayes is a class of simple classifier based on Bayes' Rule and strong (or naive) independence assumptions between features. In this project, we are going to implement a Naive Bayes Classifier for the Census Income Data Set from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/). The goal of the project is to predict whether a person makes over 50K a year based on 14 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Before getting started, you'll need to install various packages that will be used in this project. It is strongly recommanded to use Anaconda since all the open source packages can be individually installed from the Anaconda repository. By calling\n",
    "\n",
    "$ pip install [packages you want to install]\n",
    "\n",
    "Or\n",
    "\n",
    "$ conda install [packages you want to install]\n",
    "\n",
    "Anaconda compiles and builds all the packages in the Anaconda repository itself. The packages required are listed as follow.\n",
    "\n",
    "You can also refer to requirements.txt to figure out the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset consists 32561 instances, each representing an individual. The goal is to predict whether a person makes over 50K a year based on 14 features. The features are:\n",
    "\n",
    "| column | type | description |\n",
    "| --- |:---:|:--- |\n",
    "| age | continuous | trips around the sun to date\n",
    "| final_weight | continuous | census weight attribute; constructed from the original census data |\n",
    "| education_num | continuous | numeric education scale -- their maximum educational level as a number |\n",
    "| capital_gain | continuous | income from investment sources |\n",
    "| capital_loss | continuous | losses from investment sources |\n",
    "| hours_per_week | continuous | number of hours worked every week |\n",
    "| work_class | categorical | `Private`, `Self-emp-not-inc`, `Self-emp-inc`, `Federal-gov`, `Local-gov`, `State-gov`, `Without-pay`, `Never-worked` |\n",
    "| education | categorical | `Bachelors`, `Some-college`, `11th`, `HS-grad`, `Prof-school`, `Assoc-acdm`, `Assoc-voc`, `9th`, `7th-8th`, `12th`, `Masters`, `1st-4th`, `10th`, `Doctorate`, `5th-6th`, `Preschool` |\n",
    "| marital_status | categorical | `Married-civ-spouse`, `Divorced`, `Never-married`, `Separated`, `Widowed`, `Married-spouse-absent`, `Married-AF-spouse` |\n",
    "| occupation | categorical | `Tech-support`, `Craft-repair`, `Other-service`, `Sales`, `Exec-managerial`, `Prof-specialty`, `Handlers-cleaners`, `Machine-op-inspct`, `Adm-clerical`, `Farming-fishing`, `Transport-moving`, `Priv-house-serv`, `Protective-serv`, `Armed-Forces` |\n",
    "| relationship | categorical | `Wife`, `Own-child`, `Husband`, `Not-in-family`, `Other-relative`, `Unmarried.` |\n",
    "| race | categorical | `White`, `Asian-Pac-Islander`, `Amer-Indian-Eskimo`, `Other`, `Black` |\n",
    "| sex | categorical | `Female`, `Male` |\n",
    "| native_country | categorical | (41 values not shown here) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "As the data is from UCI repository, it is already quite clean. However, some instances contain missing `occupation`, `native_country` or `work_class` (represented as ? in the CSV file) and these have to be discarded from the training set. Also, it would be better rename the `income` column as `label` since we are implementing a classification algorithm here. Replace the value with 1 if `income` is `>50K` and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(fn):\n",
    "    with gzip.open(fn, \"rt\", newline='', encoding=\"UTF-8\") as file:\n",
    "        return pd.read_csv(file)\n",
    "\n",
    "\n",
    "# loads and processes data\n",
    "def load_data(file_name=\"census.csv.gz\"):\n",
    "    df = read_csv(file_name)\n",
    "    df = df[df['occupation'].eq(\"?\").eq(False)]\n",
    "    df = df[df['native_country'].eq(\"?\").eq(False)]\n",
    "    df = df[df['work_class'].eq(\"?\").eq(False)]\n",
    "    \n",
    "    \n",
    "    df.loc[df['income'].eq(\">50K\"), 'income'] = 1\n",
    "    df.loc[df['income'] != 1, 'income'] = 0\n",
    "    df.rename(columns = {\"income\":\"label\"}, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Naive Bayes classifier\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_k$ be the $k$ features of a dataset, with class label given by the variable $y$. A probabilistic classifier assigns the most probable class to each instance $(x_1,\\ldots,x_k)$, as expressed by\n",
    "$$ \\hat{y} = \\arg\\max_y P(y\\ \\mid\\ x_1,\\ldots,x_k) $$\n",
    "\n",
    "Using Bayes' theorem, the above *posterior probability* can be rewritten as\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) = \\frac{P(y) P(x_1,\\ldots,x_n\\ \\mid\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "\n",
    "\n",
    "Naive Bayes classifiers assume that the feature values are conditionally independent given the class label, that is,\n",
    "$ P(x_1,\\ldots,x_n\\ \\mid\\ y) = \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y) $. This strong assumption helps simplify the expression for posterior probability to\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) = \\frac{P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "\n",
    "For a given input $(x_1,\\ldots,x_k)$, $P(x_1,\\ldots,x_k)$ is constant. \n",
    "\n",
    "Thus, the class of a new instance can be predicted as:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y)$$\n",
    "\n",
    "\n",
    "Observe that this is the product of $k+1$ probability values, which can result in very small numbers. When working with real-world data, this often leads to an arithmetic underflow. We will instead be adding the logarithm of the probabilities:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y \\underbrace{\\log P(y)}_\\text{log-prior} + \\underbrace{\\sum_{i=1}^{k} \\log P(x_i\\ \\mid\\ y)}_\\text{log-likelihood}$$\n",
    "\n",
    "The rest of the sections deal with how each of these probability distributions -- $P(y), P(x_1\\ \\mid\\ y), \\ldots, P(x_k\\ \\mid\\ y)$ -- are estimated from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Predictor\n",
    "\n",
    "Naive Bayes classifiers are popular because we can independently model each feature and mix-and-match model types based on the prior knowledge. For example, we might know (or assume) that $(X_i|y)$ has some distribution, so we can directly use the probability density or mass function of the distribution to model $(X_i|y)$.\n",
    "\n",
    "We are going to use two classes of likelihood models:\n",
    "- Gaussian models, for continuous real-valued features (parameterized by mean $\\mu$ and variance $\\sigma$)\n",
    "- Categorical models, for features in discrete categories (parameterized by $\\mathbf{p} = <p_0,p_1\\ldots>$, one parameter per category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Feature Predictor\n",
    "\n",
    "The Gaussian distribution is characterized by two parameters - mean $\\mu$ and standard deviation $\\sigma$:\n",
    "$$ f_Z(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp{(-\\frac{(z-\\mu)^2}{2\\sigma^2})} $$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from the above distribution, the MLE for mean and standard deviation are:\n",
    "$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{n} z_j $$\n",
    "\n",
    "$$ \\hat{\\sigma} = \\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} (z_j-\\hat{\\mu})^2} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature predictor for a normally distributed real-valued, continuous feature.\n",
    "class GaussianPredictor:\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.mu = []\n",
    "        self.sigma = []\n",
    "\n",
    "        \n",
    "    # update predictor statistics (mu, sigma) for Gaussian distribution\n",
    "    def fit(self, x, y):\n",
    "        x, y = list(x), list(y)\n",
    "        x, y = pd.Series(x), pd.Series(y)\n",
    "        df = pd.concat([x, y], axis=1)\n",
    "        df.rename(columns={0:'x',1:'y'}, inplace=True)\n",
    "        exp = df.groupby(['y']).mean()\n",
    "        exp = np.array(exp['x'])\n",
    "        std = df.groupby(['y']).std(ddof=0)\n",
    "        std = np.array(std['x'])        \n",
    "        self.mu = exp\n",
    "        self.sigma = std\n",
    "        \n",
    "        return self\n",
    "\n",
    "    # log likelihood of feature values x according to each class               \n",
    "    def partial_log_likelihood(self, x):\n",
    "        result = []\n",
    "        for loc, scale in list(zip(self.mu, self.sigma)):\n",
    "            row = stats.norm.logpdf(x, loc, scale)\n",
    "            result.append(row)\n",
    "        \n",
    "        return np.array(result)\n",
    "\n",
    "\n",
    "def gaussian_pred(k):\n",
    "    return GaussianPredictor(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method in our GaussianPredictor class can be descriped as follow:\n",
    "\n",
    "- `fit()`: Learn parameters for the likelihood model using an appropriate Maximum Likelihood Estimator.\n",
    "- `partial_log_likelihood()`: Use the previously learnt parameters to compute the probability density or mass of a given feature value, and return the natural logarithm of this value.\n",
    "\n",
    "We are going to construct our Gaussian feature predictor at last on the census income dataset by calling the previous two functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature Predictor\n",
    "\n",
    "The categorical distribution with $l$ possible values $\\{\\text A, \\text B, \\text C, \\ldots\\}$ is characterized by the probability distribution $\\mathbf p$ over these values. ($\\mathbf{p} = (p_0,\\dots,p_{l-1})$ where $\\sum\\mathbf p = 1$.)\n",
    "\n",
    "If $C$ is categorically distributed, the probability of observing a particular value $z$ is:\n",
    "\n",
    "$$ \\Pr(C=z; \\mathbf{p}) = \\begin{cases}\n",
    "    p_0 & \\text{ if } z=0\n",
    "\\\\  p_1 & \\text{ if } z=1\n",
    "\\\\  \\vdots\n",
    "\\\\  p_{l-1} & \\text{ if } z=(l-1)\n",
    "\\end{cases}$$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from $C$, the smoothed Maximum Likelihood Estimator for $\\mathbf p$ is:\n",
    "$$ \\hat{p_t} = \\frac{\\sum_{j=1}^{n} [z_j=t] + \\alpha}{n + l\\alpha} $$\n",
    "\n",
    "The term in the numerator $\\sum_{j=1}^{n} [z_j=t]$ is the number of times the value $t$ occurred in the sample. To avoid the zero-count problem, smoothing is done over all possible values that may be generated by $C$. \n",
    "\n",
    "We are going to write a predictor that learns a different categorical distribution $C_i$ for each of $k$ possible classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice\n",
    "\n",
    "  1. We maintain a dictionary from each possible input token (i.e. each value) to an array of length $k$ that contains $(\\Pr(C_0=z), \\Pr(C_1=z), ..., \\Pr(C_{k-1}=z))$.\n",
    "  2. For the purpose of smoothing, we assume that all distributions can produce each value. That is, the set of possible values is the same for all $C_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature predictor for a categorical feature.\n",
    "class CategoricalPredictor:\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.p = None\n",
    "        \n",
    "\n",
    "    # initializes the predictor statistics (p) for Categorical distribution\n",
    "    def fit(self, x, y, alpha=1.):\n",
    "        x, y = list(x), list(y)\n",
    "        x, y, count = pd.Series(x), pd.Series(y), pd.Series([0]*len(x))\n",
    "        df = pd.concat([x, y, count], axis=1)\n",
    "        df = df.rename(columns={0:'x',1:'y',2:'count'})\n",
    "        n = list(df.groupby(['y']).size())\n",
    "        l = [len(df['x'].unique())] * self.k\n",
    "        \n",
    "        result = {}\n",
    "        category = df['x'].unique()\n",
    "        for c in category:\n",
    "            result[c] = [0] * self.k\n",
    "            \n",
    "        # Get the count\n",
    "        gps = df.groupby(['y', 'x']).groups\n",
    "                \n",
    "        for key in gps:\n",
    "            gps[key] = len(gps[key])\n",
    "        \n",
    "        # Assign number to count list\n",
    "        for k in gps:\n",
    "            if k[1] in result:\n",
    "                result[k[1]][k[0]] = gps[k]\n",
    "        \n",
    "        for k in result:\n",
    "            nominator = np.array(result[k]) + alpha\n",
    "            denominator = np.array(n) + np.array(l)*alpha\n",
    "            result[k] = nominator / denominator\n",
    "        \n",
    "        self.p = result\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # log likelihood of feature values x according to each class\n",
    "    def partial_log_likelihood(self, x):\n",
    "        result = []\n",
    "        for v in list(x):\n",
    "            result.append(np.log(np.array(self.p[v])))\n",
    "        \n",
    "        return np.array(result).T\n",
    "        \n",
    "\n",
    "def categorical_pred(k):\n",
    "    return CategoricalPredictor(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Same as Gaussian feature predictor, we are going to construct our categorical feature predictor at last on the census income dataset by calling `fit()` and `partial_log_likelihood()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income prediction\n",
    "\n",
    "The final step is to construct a Naive Bayers classifier using the predictor class we wrote before. The methods to be implemented in the Naive Bayers class are descriped as follow:\n",
    "\n",
    "- `__init__()`: Compute the log prior for each class and initialize the feature predictors (based on feature type). The smoothed prior for class $t$ is given by\n",
    "$$ \\text{prior}(t) = \\frac{n_t + \\alpha}{n + k\\alpha} $$\n",
    "where $n_t = \\sum_{j=1}^{n} [y_j=t]$, (i.e., the number of times the label $t$ occurred in the sample), $n$ is the number fo entries in the sample, and $k$ is the number of label values. \n",
    "- `log_likelihood()`: Compute the sum of the log prior and partial log likelihoods for all features. Use it to predict the final class label.\n",
    "- `predict()`: Use the output of log_likelihood to predict a class label; break ties by predicting the class with lower id.\n",
    "\n",
    "We then feed the classifer with the income dataset, to predict the class that each instance might belongs to. We use the classification error rate to evaluate the accuracy of our prediction compared to the true class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8274318679132684"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Naive Bayes classifier for a mixture of continuous and categorical attributes.\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    # initialize predictors for each feature and compute class prior\n",
    "    def __init__(self, df, alpha=1.):\n",
    "        gps = df.groupby(['label']).groups\n",
    "        for group in gps:\n",
    "            gps[group] = len(gps[group])\n",
    "        n = len(df['label'])\n",
    "        k = len(gps)\n",
    "        \n",
    "        self.log_prior = []\n",
    "        for group in gps:\n",
    "            nominator = gps[group] + alpha\n",
    "            denominator = n + k*alpha\n",
    "            self.log_prior.append(np.log(nominator / denominator))\n",
    "        self.log_prior = np.asarray(self.log_prior)\n",
    "        \n",
    "        self.predictor = {}\n",
    "        labels = list(df['label'])\n",
    "        feature_cols = list(df.drop(\"label\", axis=\"columns\").columns)\n",
    "        for col in feature_cols:\n",
    "            feature = list(df[col])\n",
    "            if df[col].dtype == \"object\":\n",
    "                predictor = CategoricalPredictor(k)\n",
    "            elif df[col].dtype == \"int64\":\n",
    "                predictor = GaussianPredictor(k)\n",
    "            else:\n",
    "                raise Exception()\n",
    "            predictor.fit(feature, labels)\n",
    "            self.predictor[col] = predictor\n",
    "        \n",
    "        \n",
    "    # log_likelihood for input instances from log_prior and partial_log_likelihood of feature predictors\n",
    "    def log_likelihood(self, x):\n",
    "        array = np.zeros((len(self.log_prior), len(x)))\n",
    "        cols = list(x.columns)\n",
    "        for col in cols:\n",
    "            if col != \"label\":\n",
    "                array += self.predictor[col].partial_log_likelihood(x[col])\n",
    "        result = self.log_prior[:,None] + array\n",
    "        return result\n",
    "        \n",
    "              \n",
    "    # predicts label for input instances, breaks ties in favor of the class with lower id.\n",
    "    def predict(self, x):\n",
    "        columns = x.columns\n",
    "        if \"label\" in list(columns):\n",
    "            x = x.drop(\"label\", axis=\"columns\")\n",
    "        # Select the maximum likelyhood\n",
    "        return self.log_likelihood(x).argmax(axis=0)\n",
    "        \n",
    "\n",
    "def naive_bayes(*args, **kwargs):\n",
    "    return NaiveBayesClassifier(*args, **kwargs)\n",
    "\n",
    "\n",
    "df = load_data()\n",
    "cl = naive_bayes(df)\n",
    "lp = cl.predict(df.drop(\"label\", axis=\"columns\"))\n",
    "\n",
    "np.mean(np.array(df['label']) == np.array(lp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Finally we achieve accuracy of 82% in predicting whether a person makes over 50K a year based on 14 features. Notice that one of the most important assumptions that Naive Bayes makes is that all of the features are independent. However, we haven't tested the correlation for our features. The accuracy could be further improved if we drop those columns that have high correlation with each other, or we can add an interaction feature to account for that correlation. Another potential issue that might occurs is overfitting. It is hard to determine whether 14 features is enough or we need add more. If the computation power is allowed, we may want to start trying some combinations starting from a certain number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Pandas: https://pandas.pydata.org/\n",
    "2. Numpy: https://numpy.org/\n",
    "3. Scipy: https://www.scipy.org/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
